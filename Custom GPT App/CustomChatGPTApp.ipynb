{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saatvik Sandal\\AppData\\Local\\Temp\\ipykernel_47456\\75148711.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature = 1)\n",
      "C:\\Users\\Saatvik Sandal\\AppData\\Local\\Temp\\ipykernel_47456\\75148711.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n",
      "C:\\Users\\Saatvik Sandal\\AppData\\Local\\Temp\\ipykernel_47456\\75148711.py:20: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Prompt: what were the questions i last asked?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Answer the questions normally, but respond with double spaces between everyword.\n",
      "Human: gimme info on lightspeed in a vaccum\n",
      "AI: Sure.  \n",
      "\n",
      "The speed  of  light  in  a  vacuum  is  approximately  299,792,458  meters  per  second.  \n",
      "\n",
      "It  is  a  fundamental  constant  of  nature  and  is  denoted  by  the  symbol  \"c\".  \n",
      "\n",
      "Light  travels  at  this  speed  in  a  vacuum  and  nothing  with  mass  can  travel  faster  than  light.  \n",
      "\n",
      "This  speed  plays  a  crucial  role  in  many  areas  of  physics,  including  special  relativity  and  quantum  mechanics.\n",
      "Human: how long does it take for light to travel in a vaccum?\n",
      "AI: Light  travels  approximately  299,792,458  meters  per  second  in  a  vacuum.  \n",
      "\n",
      "To  calculate  the  time  it  takes  for  light  to  travel  a  certain  distance,  you  can  use  the  formula  time  equals  distance  divided  by  speed.  \n",
      "\n",
      "For  example,  it  takes  light  about  one  second  to  travel  to  the  moon,  which  is  approximately  384,400  kilometers  away.  \n",
      "\n",
      "For  longer  distances,  such  as  to  distant  stars,  light  travel  time  can  range  from  years  to  millions  of  years.\n",
      "Human: what were the questions i last asked?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'content': 'what were the questions i last asked?', 'chat_history': [HumanMessage(content='gimme info on lightspeed in a vaccum', additional_kwargs={}, response_metadata={}), AIMessage(content='Sure.  \\n\\nThe speed  of  light  in  a  vacuum  is  approximately  299,792,458  meters  per  second.  \\n\\nIt  is  a  fundamental  constant  of  nature  and  is  denoted  by  the  symbol  \"c\".  \\n\\nLight  travels  at  this  speed  in  a  vacuum  and  nothing  with  mass  can  travel  faster  than  light.  \\n\\nThis  speed  plays  a  crucial  role  in  many  areas  of  physics,  including  special  relativity  and  quantum  mechanics.', additional_kwargs={}, response_metadata={}), HumanMessage(content='how long does it take for light to travel in a vaccum?', additional_kwargs={}, response_metadata={}), AIMessage(content='Light  travels  approximately  299,792,458  meters  per  second  in  a  vacuum.  \\n\\nTo  calculate  the  time  it  takes  for  light  to  travel  a  certain  distance,  you  can  use  the  formula  time  equals  distance  divided  by  speed.  \\n\\nFor  example,  it  takes  light  about  one  second  to  travel  to  the  moon,  which  is  approximately  384,400  kilometers  away.  \\n\\nFor  longer  distances,  such  as  to  distant  stars,  light  travel  time  can  range  from  years  to  millions  of  years.', additional_kwargs={}, response_metadata={})], 'text': 'You  last  asked  about  lightspeed  in  a  vacuum  and  how  long  it  takes  for  light  to  travel  in  a  vacuum.'}\n",
      "--------------------------------------------------\n",
      "Your Prompt: quit\n",
      "goodbye!\n"
     ]
    }
   ],
   "source": [
    "# create LLM oject\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature = 1)\n",
    "\n",
    "# create a history object to store past prompt and history for later use\n",
    "history = FileChatMessageHistory('./chat_history.json')\n",
    "\n",
    "# create a conversationBufferMemory object, which basically makes an object for the memory object that is saved for the conversation\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key = 'chat_history',\n",
    "    chat_memory=history,\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "# prompt template, input variables just being content (user content), and with each prompt include a system message,\n",
    "# include the chat history, and then the human message under the content variable\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables = ['content'],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"Answer the questions normally, but respond with double spaces between everyword.\"),\n",
    "        MessagesPlaceholder(variable_name='chat_history'), # where memory will be stored\n",
    "        HumanMessagePromptTemplate.from_template('{content}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the chain object, verbose true for seeing more details\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# loop to get continous questions and answers.\n",
    "while True:\n",
    "    content = input(\"Your Prompt: \")\n",
    "    if content in ['quit', 'exit', 'bye']:\n",
    "        print('goodbye!')\n",
    "        break\n",
    "\n",
    "    response = chain.invoke({'content' : content})\n",
    "    print(response)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
